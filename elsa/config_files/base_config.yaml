General:
  source_root: /home/eblanov/proymam2/Mammography/mamo_holistic
  gpu_type: "RTX 3090" 

Datamodule:
  num_classes: 5
  batch_size: 35
  num_workers: 16
  train_set:
    convert_to_rgb: True
    split_csv: resources/ddsm/DDSM_train.csv
    ddsm_annotations: resources/ddsm/ddsm_annotations.json.gz
    patch_size: 224
    ddsm_root: /home/Data/mamo/DDSM_png_16bit_1120x896 # he cambiado DDSM_png_1152x896 a DDSM_png_16bit_1120x896
    pin_memory: True

  val_set:
    eval_patches_root: /home/Data/mamo/eval_patches_16bits_224_test_nikulin # /home/alalbiol/Data/mamo/DDSM_eval_patches/eval_patches_224
    subset_size_test: 100
    subset_size_train: 1000



Logger:
  type: wandb
  project: ddsm_patch
  name: patches_224
  save_dir: /tmp/logs/ddsm_patch_224

LightningModule:
  model_name: resnet18
  learning_rate: 0.001
  optimizer_type: adam
  num_classes: 5
  lr_scheduler: ReduceLROnPlateau
  lr_scheduler_options:
    mode: min
    factor: 0.5
    patience: 20
    min_lr: 1.e-8

Trainer:
  max_epochs: 200



Callbacks:
  # EarlyStopping:
  #   monitor: val_loss
  #   mode: min
  #   patience: 60
  #   verbose: True
  LearningRateMonitor:
    logging_interval: 'epoch'
  # ModelCheckpoint:
  #   monitor: val_loss
  #   mode: min
  #   save_top_k: 1
  #   dirpath: /tmp/logs/ddsm_patch_224
  #   filename: best_model
  #   verbose: True
  
  