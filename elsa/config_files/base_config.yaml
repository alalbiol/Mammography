General:
  source_root: /home/eblanov/proymam2/Mammography/elsa
  gpu_type: "RTX 5090" 

Datamodule:
  num_classes: 5
  batch_size: 60
  num_workers: 16
  train_set:
    ddsm_root: /home/Data/mamo/DDSM_png_16bit_1120x896 # he cambiado DDSM_png_1152x896 a DDSM_png_16bit_1120x896
    split_csv: resources/ddsm/DDSM_train.csv
    ddsm_annotations: resources/ddsm/ddsm_annotations_16bits_1120_896.json.gz
    patch_size: 224
    convert_to_rgb: True
    include_normals: False
    normalize_input: True
    pin_memory: True

  val_set:
    eval_patches_root: /home/Data/mamo/eval_patches_16bits_224_test_nikulin # /home/alalbiol/Data/mamo/DDSM_eval_patches/eval_patches_224
    #subset_size_test: 100
    #subset_size_train: 1000



Logger:
  type: wandb
  project: ddsm_patch
  name: patches_224
  save_dir: /home/eblanov/tmp/logs/ddsm_patch_224

LightningModule:
  model_name: resnet18
  learning_rate: 0.001
  optimizer_type: adam
  num_classes: 5
  lr_scheduler: ReduceLROnPlateau
  lr_scheduler_options:
    mode: min
    factor: 0.5
    patience: 20
    min_lr: 1.e-8

Trainer:
  max_epochs: 100
  #limit_train_batches: 10
  #limit_val_batches: 10
  



Callbacks:
  # EarlyStopping:
  #   monitor: val_loss
  #   mode: min
  #   patience: 60
  #   verbose: True
  LearningRateMonitor:
    logging_interval: 'epoch'
  # ModelCheckpoint:
  #   monitor: val_loss
  #   mode: min
  #   save_top_k: 1
  #   dirpath: /tmp/logs/ddsm_patch_224
  #   filename: best_model
  #   verbose: True
  
  